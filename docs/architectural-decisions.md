# Architectural Decisions

> Understanding the "Why" behind every technology choice

This document explains the reasoning behind major architectural decisions made in the Wallet Service. Each decision was carefully evaluated considering performance, scalability, maintainability, and team expertise.

## üéØ Decision Framework

Every architectural decision follows this framework:
- **Context**: What problem are we solving?
- **Options**: What alternatives did we consider?
- **Decision**: What did we choose and why?
- **Consequences**: What are the trade-offs?

---

## üèóÔ∏è Framework Choice: Quarkus

### Context
We needed a Java framework that could deliver:
- **Fast startup times** for cloud deployment
- **Low memory footprint** for cost efficiency
- **High throughput** for financial transactions
- **Developer productivity** for rapid iteration

### Options Considered

| Framework | Startup Time | Memory Usage | Throughput | Learning Curve |
|-----------|--------------|--------------|------------|----------------|
| **Quarkus** | ~1s | ~100MB | Very High | Medium |
| Spring Boot | ~10s | ~300MB | High | Low |
| Micronaut | ~3s | ~150MB | High | Medium |
| Helidon | ~5s | ~200MB | Medium | High |

### Decision: Quarkus

**Why Quarkus won:**
- ‚úÖ **Native compilation** - Sub-second startup times
- ‚úÖ **Reactive-first** - Built for high concurrency
- ‚úÖ **Memory efficient** - 3x less memory than Spring Boot
- ‚úÖ **Developer experience** - Live reload, dev UI
- ‚úÖ **Cloud native** - Kubernetes optimized
- ‚úÖ **Standards-based** - Uses familiar Jakarta EE APIs

### Consequences
- **Positive**: Excellent performance, lower cloud costs, fast deployments
- **Negative**: Smaller ecosystem compared to Spring, steeper learning curve
- **Mitigation**: Team training, gradual adoption, community support

---

## üèõÔ∏è Architecture Pattern: CQRS

### Context
Financial applications require:
- **High read performance** for balance queries
- **Strong consistency** for transactions
- **Audit trails** for compliance
- **Scalability** for growth

### Options Considered

#### Traditional CRUD
- **Pros**: Simple, familiar, single data model
- **Cons**: Read/write coupling, scaling bottlenecks, complex queries

#### Event Sourcing Only
- **Pros**: Complete audit trail, time travel queries
- **Cons**: Complex queries, eventual consistency challenges

#### **CQRS (Chosen)**
- **Pros**: Optimized reads/writes, scalable, flexible
- **Cons**: Increased complexity, data synchronization

### Decision: CQRS with Event Sourcing

```mermaid
graph LR
    subgraph "Command Side"
        CMD[Commands] --> HANDLER[Command Handlers]
        HANDLER --> EVENTS[Events]
        EVENTS --> STORE[(Event Store)]
    end
    
    subgraph "Query Side"
        QUERY[Queries] --> READ_MODEL[(Read Models)]
        EVENTS --> READ_MODEL
    end
    
    CLIENT[Client] --> CMD
    CLIENT --> QUERY
```

**Why CQRS:**
- ‚úÖ **Separate scaling** - Scale reads and writes independently
- ‚úÖ **Optimized models** - Different models for different use cases
- ‚úÖ **Performance** - Denormalized read models for speed
- ‚úÖ **Flexibility** - Easy to add new query models
- ‚úÖ **Audit trail** - Events provide complete history

### Implementation Details

```java
// Command Side
@ApplicationScoped
public class DepositFundsCommandHandler {
    public Uni<String> handle(DepositFundsCommand command) {
        return walletRepository.findById(command.getWalletId())
            .chain(wallet -> {
                wallet.deposit(command.getAmount());
                return walletRepository.persist(wallet)
                    .chain(() -> eventPublisher.publish(
                        new FundsDepositedEvent(wallet.getId(), command.getAmount())
                    ));
            });
    }
}

// Query Side
@ApplicationScoped
public class GetBalanceQueryHandler {
    public Uni<BalanceResponse> handle(GetBalanceQuery query) {
        return balanceReadModel.findById(query.getWalletId())
            .map(balance -> new BalanceResponse(balance));
    }
}
```

---

## üóÑÔ∏è Database Strategy: Primary-Replica Setup

### Context
Financial applications need:
- **Strong consistency** for writes
- **High availability** for reads
- **Disaster recovery** capabilities
- **Read scalability** for analytics

### Options Considered

#### Single Database
- **Pros**: Simple, strong consistency
- **Cons**: Single point of failure, read/write contention

#### Microservice per Database
- **Pros**: Service isolation, technology diversity
- **Cons**: Distributed transactions, data consistency challenges

#### **Primary-Replica (Chosen)**
- **Pros**: Read scalability, high availability, backup
- **Cons**: Replication lag, complexity

### Decision: MySQL Primary-Replica

```mermaid
graph TB
    subgraph "Application Layer"
        APP[Wallet Service]
    end
    
    subgraph "Database Layer"
        PRIMARY[(MySQL Primary<br/>Writes Only)]
        REPLICA1[(MySQL Replica 1<br/>Reads Only)]
        REPLICA2[(MySQL Replica 2<br/>Reads Only)]
    end
    
    APP -->|Writes| PRIMARY
    APP -->|Reads| REPLICA1
    APP -->|Reads| REPLICA2
    
    PRIMARY -.->|Replication| REPLICA1
    PRIMARY -.->|Replication| REPLICA2
```

**Why Primary-Replica:**
- ‚úÖ **Write consistency** - All writes go to primary
- ‚úÖ **Read scalability** - Multiple replicas handle reads
- ‚úÖ **High availability** - Replica promotion on primary failure
- ‚úÖ **Backup strategy** - Replicas serve as live backups
- ‚úÖ **Geographic distribution** - Replicas in different regions

### Configuration

```java
// Write operations use primary
@ReactiveDataSource("write")
WalletRepository walletWriteRepository;

// Read operations use replica
@ReactiveDataSource("read") 
WalletReadRepository walletReadRepository;
```

---

## üì® Messaging: Apache Kafka

### Context
We needed reliable messaging for:
- **Event publishing** after transactions
- **Audit logging** for compliance
- **Integration** with external systems
- **Eventual consistency** between services

### Options Considered

| Solution | Throughput | Durability | Ordering | Complexity |
|----------|------------|------------|----------|------------|
| **Kafka** | Very High | Excellent | Partition-level | Medium |
| RabbitMQ | High | Good | Queue-level | Low |
| AWS SQS | Medium | Excellent | FIFO queues | Low |
| Redis Streams | High | Good | Stream-level | Low |

### Decision: Apache Kafka

**Why Kafka:**
- ‚úÖ **High throughput** - Millions of messages per second
- ‚úÖ **Durability** - Persistent, replicated storage
- ‚úÖ **Ordering** - Partition-level ordering guarantees
- ‚úÖ **Scalability** - Horizontal scaling with partitions
- ‚úÖ **Ecosystem** - Rich connector ecosystem
- ‚úÖ **Event sourcing** - Natural fit for event storage

### Event Flow

```mermaid
sequenceDiagram
    participant Client
    participant WalletService
    participant Database
    participant Kafka
    participant AuditService
    
    Client->>WalletService: Deposit Request
    WalletService->>Database: Update Balance
    WalletService->>Kafka: Publish DepositEvent
    WalletService-->>Client: Success Response
    Kafka->>AuditService: Consume Event
    AuditService->>AuditService: Log Transaction
```

---

## üöÄ Caching Strategy: Redis

### Context
Financial applications require:
- **Fast balance lookups** (< 50ms)
- **Rate limiting** for security
- **Session management** for web clients
- **Distributed locking** for consistency

### Options Considered

#### In-Memory Caching (Caffeine)
- **Pros**: Very fast, no network overhead
- **Cons**: Not shared, memory limited, no persistence

#### **Redis (Chosen)**
- **Pros**: Shared cache, persistent, rich data types
- **Cons**: Network latency, additional infrastructure

#### Hazelcast
- **Pros**: Distributed, in-memory grid
- **Cons**: Complex setup, memory intensive

### Decision: Redis Cluster

**Why Redis:**
- ‚úÖ **Performance** - Sub-millisecond latency
- ‚úÖ **Scalability** - Cluster mode for horizontal scaling
- ‚úÖ **Persistence** - Optional durability
- ‚úÖ **Data structures** - Lists, sets, hashes, streams
- ‚úÖ **Features** - Pub/sub, transactions, Lua scripts

### Caching Strategy

```java
@ApplicationScoped
public class WalletStateCache {
    
    @CacheResult(cacheName = "wallet-balance")
    public Uni<BigDecimal> getBalance(String walletId) {
        return redisClient.get(walletId)
            .map(value -> new BigDecimal(value));
    }
    
    @CacheInvalidate(cacheName = "wallet-balance")
    public Uni<Void> invalidateBalance(String walletId) {
        return redisClient.del(walletId);
    }
}
```

---

## üß™ Testing Strategy: Comprehensive Quality

### Context
Financial software requires:
- **High confidence** in correctness
- **Regression prevention** during changes
- **Performance validation** under load
- **Security verification** against threats

### Testing Pyramid

```mermaid
graph TB
    subgraph "Testing Pyramid"
        E2E[End-to-End Tests<br/>~5%]
        INTEGRATION[Integration Tests<br/>~20%]
        UNIT[Unit Tests<br/>~75%]
    end
    
    subgraph "Quality Gates"
        MUTATION[Mutation Testing<br/>PIT - 100% Score]
        LOAD[Load Testing<br/>10K Concurrent Users]
        SECURITY[Security Testing<br/>OWASP Compliance]
    end
```

### Decision: Multi-Layer Testing

**Testing Tools:**
- ‚úÖ **Unit Tests** - JUnit 5 + Mockito
- ‚úÖ **Integration Tests** - Testcontainers + Quarkus Test
- ‚úÖ **Mutation Testing** - PIT (100% mutation score)
- ‚úÖ **Load Testing** - JMeter + K6
- ‚úÖ **Security Testing** - OWASP ZAP

---

## üåê Deployment: Cloud-Native

### Context
Modern applications need:
- **Scalability** for varying loads
- **Resilience** against failures
- **Observability** for operations
- **Cost efficiency** for sustainability

### Decision: Kubernetes + Docker

**Why Kubernetes:**
- ‚úÖ **Container orchestration** - Automated deployment and scaling
- ‚úÖ **Service discovery** - Built-in load balancing
- ‚úÖ **Health checks** - Automatic restart of failed pods
- ‚úÖ **Rolling updates** - Zero-downtime deployments
- ‚úÖ **Resource management** - CPU and memory limits
- ‚úÖ **AWS optimized** - Designed for AWS services and infrastructure

### Deployment Architecture

```yaml
# Kubernetes Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wallet-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wallet-service
  template:
    spec:
      containers:
      - name: wallet-service
        image: wallet-service:latest
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
```

---

## üìä Monitoring: Observability-First

### Context
Production systems require:
- **Real-time metrics** for performance monitoring
- **Distributed tracing** for debugging
- **Log aggregation** for troubleshooting
- **Alerting** for proactive response

### Decision: Prometheus + Grafana + OpenTelemetry

**Why This Stack:**
- ‚úÖ **Prometheus** - Time-series metrics with powerful queries
- ‚úÖ **Grafana** - Rich visualization and alerting
- ‚úÖ **OpenTelemetry** - Vendor-neutral observability
- ‚úÖ **Jaeger** - Distributed tracing for microservices

### Observability Stack

```mermaid
graph TB
    subgraph "Application"
        APP[Wallet Service<br/>OpenTelemetry]
    end
    
    subgraph "Metrics"
        PROMETHEUS[Prometheus]
        GRAFANA[Grafana]
    end
    
    subgraph "Tracing"
        JAEGER[Jaeger]
    end
    
    subgraph "Logging"
        ELK[ELK Stack]
    end
    
    APP --> PROMETHEUS
    APP --> JAEGER
    APP --> ELK
    
    PROMETHEUS --> GRAFANA
```

---

## üîÑ Migration Strategy: Gradual Evolution

### Context
The project evolved from:
1. **Simple Docker setup** ‚Üí **Kubernetes cluster**
2. **Single database** ‚Üí **Primary-replica setup**
3. **Basic testing** ‚Üí **Comprehensive quality gates**

### Decision: Incremental Migration

**Migration Phases:**
1. ‚úÖ **Phase 1** - Basic functionality with Docker
2. ‚úÖ **Phase 2** - CQRS implementation
3. ‚úÖ **Phase 3** - Database replication
4. ‚úÖ **Phase 4** - Event sourcing with Kafka
5. ‚úÖ **Phase 5** - Comprehensive testing
6. üîÑ **Phase 6** - Production deployment (in progress)

**Why Gradual:**
- ‚úÖ **Risk mitigation** - Small, manageable changes
- ‚úÖ **Learning curve** - Team adapts incrementally
- ‚úÖ **Validation** - Each phase proves value
- ‚úÖ **Rollback capability** - Easy to revert if needed

---

## üéØ Summary

These architectural decisions create a system that is:

- **üöÄ Performant** - Sub-100ms response times
- **üìà Scalable** - Handles millions of transactions
- **üîí Secure** - Enterprise-grade security
- **üõ°Ô∏è Resilient** - High availability and disaster recovery
- **üß™ Reliable** - Comprehensive testing and monitoring
- **üí∞ Cost-effective** - Optimized resource usage

Each decision was made with careful consideration of trade-offs, and we continue to evaluate and evolve our architecture as requirements change and technology advances.
